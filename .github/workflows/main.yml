name: Deploy API and Dashboard

on:
    # Trigger the workflow on push
    push:
        branches: 
            # Push events on main branch
            - main

# The Job defines a series of series that execute on the same runner
jobs: 

    CI: 
        # Define the runner used in the workflow
        runs-on: ubuntu-latest
        steps:
            # Check out repo so our workflow can access it
            - uses: actions/checkout@v2

            # Step-1 Set up Python
            - name: Set up Python
                # This action sets up a Python environment for use in actions
              uses: actions/setup-python@v2
              with: 
                python-version: 3.11

            # Step-2 Install Python virtual environment (EV)
            - name: Install Python virtual environment
              run: pip3 install virtualenv

            # Step-3 Setup Virtual ENV
            # https://docs.github.com/en/actions/guides/caching-dependencies-to-speed-up-workflows
            - name: Virtual ENV
              uses: actions/cache@v2
              id: cache-venv # name for referring later
              with:
                path: vevn # what we cache: the Virtual ENV
                # The cache key depends on requirements.txt
                key: ${{ runner.os }}-venv-${{ hashFiles('**/requirements.txt') }}
                restore-keys: /
                    ${{ runner.os }}-venv-

            # Step-4 Build a Virtual ENV, but only if it doesn't already exist
            - name: Activate Virtual ENV
              run: python -m venv venv && source venv/bin/activate && pip3 install -r requirements.txt

            # Run tests
            - name: Run Tests
              # Note that you have to activate the virtualenv in every step
              # because GitHub actions doesn't preserve the environment
              run: . venv/bin/activate && pytest

    CD: 
        runs-on: ubuntu-latest
        needs: [CI]
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        steps: 
            # Check out repo so our workflow can access it
            - uses: actions/checkout@v2

            # Set up the ECS Instance
            - name: Set up EC2 instance
              env: 
                AWS_SSH_PRIVATE_KEY: ${{ secrets.AWS_SSH_PRIVATE_KEY }}
                AWS_PUBLIC_IP_ADDRESS_PREDICT_API: ${{ secrets.AWS_PUBLIC_IP_ADDRESS_PREDICT_API }}
              run: |
                echo "$AWS_SSH_PRIVATE_KEY" > private_key && chmod 600 private_key
                ssh -o StrictHostKeyChecking=no -i private_key ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API} '
                  sudo apt-get update
                  sudo apt install -y nginx docker.io
                  echo "server {
                    listen 80;
                    server_name ${AWS_PUBLIC_IP_ADDRESS_PREDICT_API};
                    location / {
                        proxy_pass http://127.0.0.1:8000;
                    }
                  }" | sudo tee /etc/nginx/sites-enabled/fastapi_nginx > /dev/null
                  sudo service nginx restart
                '

            # Steps for PREDICTION API
            - name: Copy files inside the EC2 instance 
              env: 
                AWS_SSH_PRIVATE_KEY: ${{ secrets.AWS_SSH_PRIVATE_KEY }}
                AWS_PUBLIC_IP_ADDRESS_PREDICT_API: ${{ secrets.AWS_PUBLIC_IP_ADDRESS_PREDICT_API }}
              run: |
                ssh -o StrictHostKeyChecking=no -i private_key ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API} '
                    mkdir -p credit-scoring/api
                    mkdir -p credit-scoring/data/processed
                    mkdir -p credit-scoring/models
                '
                scp -o StrictHostKeyChecking=no -i private_key -r Dockerfile ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API}:/home/ubuntu/
                scp -o StrictHostKeyChecking=no -i private_key -r api/ ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API}:/home/ubuntu/credit-scoring/
                scp -o StrictHostKeyChecking=no -i private_key -r data/processed/test_feature_engineering_encoded.csv.gz ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API}:/home/ubuntu/credit-scoring/data/processed/
                scp -o StrictHostKeyChecking=no -i private_key -r models/ ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API}:/home/ubuntu/credit-scoring/
                scp -o StrictHostKeyChecking=no -i private_key -r requirements.txt ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API}:/home/ubuntu/credit-scoring/
                
            - name: Create docker image
              env: 
                AWS_SSH_PRIVATE_KEY: ${{ secrets.AWS_SSH_PRIVATE_KEY }}
                AWS_PUBLIC_IP_ADDRESS_PREDICT_API: ${{ secrets.AWS_PUBLIC_IP_ADDRESS_PREDICT_API }}
              run: |
                ssh -o StrictHostKeyChecking=no -i private_key ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API} '
                    sudo docker build -t credit-scoring-app:latest .
                '
                
            - name: Run docker
              env: 
                AWS_SSH_PRIVATE_KEY: ${{ secrets.AWS_SSH_PRIVATE_KEY }}
                AWS_PUBLIC_IP_ADDRESS_PREDICT_API: ${{ secrets.AWS_PUBLIC_IP_ADDRESS_PREDICT_API }}
              run: |
                ssh -o StrictHostKeyChecking=no -i private_key ubuntu@${AWS_PUBLIC_IP_ADDRESS_PREDICT_API} '
                    sudo docker run -d -p 8000:8000 credit-scoring-app uvicorn prediction:app --proxy-headers --host=0.0.0.0 --port=8000
                '
            # Steps for STATS API
            